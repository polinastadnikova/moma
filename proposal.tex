\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}



\title{Methods of Mathematical Analysis final project}

\author{Polina Stadnikova}

%\date{\today}

\begin{document}
\maketitle
\begin{abstract}
This project implements a basic idea of the inverted indexing approach for creating word embeddings where words are represented as one-dimensional arrays. The objective is to obtain cross-lingual word representations that capture similar or the same concepts in different languages and to investigate potential challenges and limitations. For the evaluation, different techniques are used, including the distance between two vectors. The outcome of the project is a small tool that allows the user to perform some experiments with these embeddings.
\end{abstract}

\section{Method}
\subsection{Cross-lingual embeddings}
Cross-lingual embeddings are projections from different languages into the same semantic space. They can be used in many NLP tasks, such as machine translation, syntactic parsing, POS-tagging, and, therefore, are an object of active research in distributional semantics.\par
Creating such word embeddings usually requires multilingual parallel corpora. My method follows the idea from \cite{b} of using Wikipedia as a source. 
\subsection{Count-based representations}
The semantic space of a language can be composed by vectors that correspond to the words in this space. In count-based approaches vectors contain the information about word co-occurrences, represented by raw  or weighted counts\cite{j}.\\

\subsection{Corpus}
\subsection{Preprocessing} 
\section{Evaluation}
\section{User Guide}
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
 \newpage
\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem[S\o gaard et al.]{b}
Anders S\o gaard, Zeljko Agic, Hector Martinez Alonso, Barbara Plank, and Bernd Bohnet (2015).
\newblock Inverted indexing for cross-lingual NLP
\newblock {\em In \textit{ACL}, Vol. 1}, 1713--1722.

\bibitem[Jurafsky \& Martin]{j}
Daniel Jurafsky and James H. Martin (2017).
\newblock Speech and Language Processing.
\newblock {\em Third Edition draft}, 275--291.

\bibitem[Och \& Ney, 2004]{phrase}
Och, Franz Josef and Ney, Hermann (2004).
\newblock The Alignment Template Approach to Statistical Machine Translation.
\newblock {\em Computational Linguistics 30}, 417--449

\bibitem[Pasupat \& Liang, 2015]{float}
Pasupat, Panupong and Liang, Percy (2015).
\newblock Compositional Semantic Parsing on Semi-Structured Tables.
\newblock {\em Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},1470--1480.
 
\end{thebibliography}

%For the final project, I would like to implement the inverted indexing approach to obtain word representations. These representations can be cross-lingual and, therefore, helpful for many NLP tasks like POS-tagging, depedency parsing, word alignments, etc. I got inspired by this paper: \textit{http://www.aclweb.org/anthology/P15-1165}, which I presented in Word embeddings for NLP and IR seminar this semester.
%
%In a nutshell, each word is represented by a row in the co-occurrence  concept-word matrix. A concept is represented by some Wikipedia articles which describe the same topic in \textit{different languages} and are linked to the same node in the Wikipedia ontology.
%With the forward indexing, we would list the words per concept, but here we make use of the inverted indexing and list the concepts per word .
%
%This approach is counted-based, that means, a word is described by a vector which stores the information about how often this word is used to describe different concepts. That is, for each language, we need one co-occurrence matrix. Such matrices allow to capture relatedness between the words in different languages: words describing the same concepts have similar vectors. For example, if the German word \textit{Brille} and the English \textit{glasses} both occur in the Wiki article about Harry Potter, they will have similar representations. 
%
%The paper does not describe the approach in detail, so I think it would be challenging enough just to implement the representations. They do different experiments with these embeddings, but it would not be manageable to recreate them within 25 hours. Fortunately, they also provide the Wikipedia data here: \textit{https://sites.google.com/site/rmyeid/projects/polyglot} (which even seems to work ;)). My plan is to do the following:
%\begin{enumerate}
%\item Implement a method for constructing matrices from the given data
%\item Create matrices for some languages (preferably for the languages I speak, so I can easily evaluate my method)
%\item Compare vector representations and extract similar words (here I think of some kind of alignments: e.g., Brille and glasses are aligned together)
%\item For the evaluation, I have two ideas: 1) just automatically check whether the aligned words really have similar vectors (not sure whether this would make sense though) 2) create gold alignments manually and compare, compute recall, precision, F-score (I hope I will have time for this). 


\end{enumerate}


\end{document}